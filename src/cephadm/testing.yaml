#Bootstrap:
image:
docker: false
data_dir: "/var/lib/ceph"
log_dir: "/var/log/ceph"
logrotate_dir: "/etc/logrotate.d"
sysctl_dir: "/etc/sysctl.d"
unit_dir: "/etc/systemd/system"
verbose: false
log_dest:
timeout:
retry: 15
mon_ip: 171.254.95.156
initial_dashboard_user: admin
initial_dashboard_password: passowrd
ssl_dashboard_port: 8443
ssh_user: root
skip_mon_network: false
skip_dashboard: false
dashboard_password_noupdate: false
no_minimize_config: false
skip_ping_check: false
skip_pull: false
skip_firewalld: false
allow_overwrite: false
no_cleanup_on_failure: false
allow_fqdn_hostname: false
allow_mismatched_release: false
skip_prepare_host: false
orphan_initial_daemons: false
skip_monitoring_stack: true
with_centralized_logging: false
apply_spec:
shared_ceph_folder:
registry_url:
registry_username:
registry_password:
registry_json:
container_init: true
cluster_network:
single_host_defaults: false
log_to_file: false
deploy_cephadm_agent: false
custom_prometheus_alerts:
force: true
keep_logs: false
zap_osds: true
name:
keyring:
mount:
precheck_port:
  - 3300
  - 6789
precheck_devices: true
funcs:
#  - "command_rm_cluster"
#  - "command_prepare_host"
#  - "command_bootstrap"
  - "command_shell"

#Setting up cluster
setup: true
hosts:
 
- name: ceph-node-1
  ipaddresses: 171.254.95.193
  label: osd
  ssh-user: root

- name: ceph-node-2
  ipaddresses: 171.254.95.156
  label: _admin,mgr,mon,osd,rgw
  ssh-user: root

- name: ceph-node-3
  ipaddresses: 171.254.93.217
  label: osd,mgr
  ssh-user: root
services:
  monitor:
    count-per-host: 1
  manager:
    count-per-host: 1
  radosgw: 
    service_list:
      - name: public
        port: 8888
        count-per-host: 1
    user:
      - uid: test
        display_name: test
    zone:
      - name: vn-central-1a-test
        default: true
        uid: test
        zonegroup: vn-central-1-test
        storage_classes:
          default-placement: 
            storage_class: SLIVER
            data_pool: 
              name: default.rgw.sliver.data
              pg_num: 16
          hdd-placement:
            storage_class: STANDARD
            data_pool: 
              name: default.rgw.sliver.data
              pg_num: 16
            index_pool: 
              name: default.rgw.buckets.hddplacement.index
              pg_num: 16
            data_extra_pool: 
              name: default.rgw.buckets.hddplacement.index
              pg_num: 16
    zonegroup:
      - name: vn-central-1-test
        default: true
        realm: test
    realm:
      - name: test
        default: true
    
  osd_dry_run: true
  add-osds:
    - service-id: osd_spec_hdd
      placement: 
        hosts:
          - ceph-node-1
          - ceph-node-2
          - ceph-node-3
      spec:
        osds_per_device: 3
        data_devices:
          rotational: 1
          size: '100G'
  remove-osds:
    id-lists:
      - 2
      - 5
      - 8
    zap_devices: true


volume: []
no_hosts: false
dry_run: true